{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 20:20:26.038352: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-08 20:20:26.039777: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-08 20:20:26.067672: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-08 20:20:26.068173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-08 20:20:27.416308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "from waymo_open_dataset.protos import end_to_end_driving_submission_pb2 as wod_e2ed_submission_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CAMERAS = 8  # 8 cameras for 360-degree view\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "NUM_PAST_STEPS = 16  # 4 seconds of past data at 4Hz\n",
    "NUM_FUTURE_STEPS = 20  # 5 seconds of future data at 4Hz\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaymoE2EDataset(Dataset):\n",
    "    \"\"\"Dataset for Waymo End-to-End Driving Challenge.\"\"\"\n",
    "    \n",
    "    def __init__(self, tfrecord_files, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tfrecord_files: List of paths to TFRecord files\n",
    "            transform: Optional transform to be applied on images\n",
    "        \"\"\"\n",
    "        self.tfrecord_files = tfrecord_files\n",
    "        self.transform = transform\n",
    "        self.examples = self._load_examples()\n",
    "        \n",
    "    def _load_examples(self):\n",
    "        \"\"\"Load and parse examples from TFRecord files.\"\"\"\n",
    "        examples = []\n",
    "        \n",
    "        # Create dataset from TFRecord files\n",
    "        dataset = tf.data.TFRecordDataset(self.tfrecord_files, compression_type='')\n",
    "        \n",
    "        # Parse each example and store necessary information\n",
    "        for raw_example in tqdm(dataset, desc=\"Loading dataset\"):\n",
    "            data = wod_e2ed_pb2.E2EDFrame()\n",
    "            data.ParseFromString(raw_example.numpy())\n",
    "            \n",
    "            example = {\n",
    "                'frame_name': data.frame.context.name,\n",
    "                'images': [self._decode_image(img.image) for img in data.frame.images],\n",
    "                'past_states': {\n",
    "                    'pos_x': np.array(data.past_states.pos_x, dtype=np.float32),\n",
    "                    'pos_y': np.array(data.past_states.pos_y, dtype=np.float32),\n",
    "                    'vel_x': np.array(data.past_states.vel_x, dtype=np.float32) if data.past_states.vel_x else np.zeros(NUM_PAST_STEPS, dtype=np.float32),\n",
    "                    'vel_y': np.array(data.past_states.vel_y, dtype=np.float32) if data.past_states.vel_y else np.zeros(NUM_PAST_STEPS, dtype=np.float32)\n",
    "                },\n",
    "                'future_states': {\n",
    "                    'pos_x': np.array(data.future_states.pos_x, dtype=np.float32),\n",
    "                    'pos_y': np.array(data.future_states.pos_y, dtype=np.float32)\n",
    "                },\n",
    "                'intent': data.intent\n",
    "            }\n",
    "            \n",
    "            examples.append(example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def _decode_image(self, image_bytes):\n",
    "        \"\"\"Decode image bytes to numpy array.\"\"\"\n",
    "        image = tf.io.decode_image(image_bytes).numpy()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Stack images for batch processing\n",
    "        images = torch.stack([torch.from_numpy(img).permute(2, 0, 1).float() / 255.0 \n",
    "                             for img in example['images']])\n",
    "        \n",
    "        # Create past trajectory tensor [past_steps, features]\n",
    "        past_trajectory = np.stack([\n",
    "            example['past_states']['pos_x'], \n",
    "            example['past_states']['pos_y'],\n",
    "            example['past_states']['vel_x'],\n",
    "            example['past_states']['vel_y']\n",
    "        ], axis=1)\n",
    "        \n",
    "        # Create future trajectory tensor [future_steps, 2] (only positions x,y)\n",
    "        future_trajectory = np.stack([\n",
    "            example['future_states']['pos_x'],\n",
    "            example['future_states']['pos_y']\n",
    "        ], axis=1)\n",
    "        \n",
    "        # One-hot encode intent\n",
    "        intent = np.zeros(4, dtype=np.float32)  # 4 for UNKNOWN, GO_STRAIGHT, GO_LEFT, GO_RIGHT\n",
    "        intent[example['intent']] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'frame_name': example['frame_name'],\n",
    "            'images': images,\n",
    "            'past_trajectory': torch.from_numpy(past_trajectory).float(),\n",
    "            'future_trajectory': torch.from_numpy(future_trajectory).float(),\n",
    "            'intent': torch.from_numpy(intent).float()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"CNN encoder for processing camera images.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dim=256):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Use ResNet18 as the backbone, removing the final classification layer\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "        # Project features to the desired output dimension\n",
    "        self.projection = nn.Linear(512, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, num_cameras, channels, height, width]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, num_cameras, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size, num_cameras = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Reshape for processing each image independently\n",
    "        x = x.view(batch_size * num_cameras, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(batch_size * num_cameras, -1)\n",
    "        \n",
    "        # Project features\n",
    "        features = self.projection(features)\n",
    "        \n",
    "        # Reshape back to [batch_size, num_cameras, output_dim]\n",
    "        features = features.view(batch_size, num_cameras, -1)\n",
    "        \n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryEncoder(nn.Module):\n",
    "    \"\"\"Encoder for processing past trajectory data.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=4, hidden_dim=128, output_dim=256):\n",
    "        super(TrajectoryEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=2, \n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.projection = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, past_steps, features]\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        # Process the trajectory sequence\n",
    "        output, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        # Use the final hidden state from both directions\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        hidden = self.projection(hidden)\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFusion(nn.Module):\n",
    "    \"\"\"Transformer for fusing multi-modal features.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=256, num_heads=8, num_layers=4):\n",
    "        super(TransformerFusion, self).__init__()\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=input_dim * 4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Stack multiple encoder layers\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, image_features, trajectory_features, intent_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_features: Tensor of shape [batch_size, num_cameras, feature_dim]\n",
    "            trajectory_features: Tensor of shape [batch_size, feature_dim]\n",
    "            intent_features: Tensor of shape [batch_size, feature_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, (num_cameras + 2), feature_dim]\n",
    "        \"\"\"\n",
    "        batch_size = image_features.shape[0]\n",
    "        feature_dim = image_features.shape[2]\n",
    "        \n",
    "        # Reshape trajectory and intent features to match image_features dimensions\n",
    "        trajectory_features = trajectory_features.view(batch_size, 1, feature_dim)\n",
    "        intent_features = intent_features.view(batch_size, 1, feature_dim)\n",
    "        \n",
    "        # Concatenate all features along the sequence dimension\n",
    "        # [batch_size, num_cameras + 2, feature_dim]\n",
    "        features = torch.cat([image_features, trajectory_features, intent_features], dim=1)\n",
    "        \n",
    "        # Apply transformer to fuse features\n",
    "        fused_features = self.transformer(features)\n",
    "        \n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDecoder(nn.Module):\n",
    "    \"\"\"Decoder for predicting future trajectory.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=256, hidden_dim=512, output_steps=20):\n",
    "        super(TrajectoryDecoder, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_steps * 2)  # 2 for (x, y) coordinates\n",
    "        )\n",
    "        \n",
    "        self.output_steps = output_steps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, feature_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, output_steps, 2]\n",
    "        \"\"\"\n",
    "        # Apply MLP to get flattened trajectory\n",
    "        trajectory = self.mlp(x)\n",
    "        \n",
    "        # Reshape to [batch_size, output_steps, 2]\n",
    "        trajectory = trajectory.view(-1, self.output_steps, 2)\n",
    "        \n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentEncoder(nn.Module):\n",
    "    \"\"\"Encoder for processing intent data.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=4, output_dim=256):\n",
    "        super(IntentEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape [batch_size, 4] (one-hot encoded intent)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EDrivingModel(nn.Module):\n",
    "    \"\"\"End-to-end driving model that predicts future trajectory.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim=256):\n",
    "        super(E2EDrivingModel, self).__init__()\n",
    "        \n",
    "        # Encoders\n",
    "        self.image_encoder = ImageEncoder(output_dim=feature_dim)\n",
    "        self.trajectory_encoder = TrajectoryEncoder(output_dim=feature_dim)\n",
    "        self.intent_encoder = IntentEncoder(output_dim=feature_dim)\n",
    "        \n",
    "        # Feature fusion\n",
    "        self.transformer = TransformerFusion(input_dim=feature_dim)\n",
    "        \n",
    "        # Trajectory prediction\n",
    "        self.decoder = TrajectoryDecoder(input_dim=feature_dim)\n",
    "        \n",
    "        # Global feature aggregation\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, images, past_trajectory, intent):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor of shape [batch_size, num_cameras, channels, height, width]\n",
    "            past_trajectory: Tensor of shape [batch_size, past_steps, features]\n",
    "            intent: Tensor of shape [batch_size, 4] (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, future_steps, 2]\n",
    "        \"\"\"\n",
    "        # Encode inputs\n",
    "        image_features = self.image_encoder(images)\n",
    "        trajectory_features = self.trajectory_encoder(past_trajectory)\n",
    "        intent_features = self.intent_encoder(intent)\n",
    "        \n",
    "        # Fuse features with transformer\n",
    "        fused_features = self.transformer(image_features, trajectory_features, intent_features)\n",
    "        \n",
    "        # Global pooling to get a single feature vector per batch\n",
    "        # [batch_size, num_features + 2, feature_dim] -> [batch_size, feature_dim]\n",
    "        fused_features = fused_features.transpose(1, 2)\n",
    "        pooled_features = self.global_pool(fused_features).squeeze(2)\n",
    "        \n",
    "        # Decode to get future trajectory\n",
    "        future_trajectory = self.decoder(pooled_features)\n",
    "        \n",
    "        return future_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            # Move data to device\n",
    "            images = batch['images'].to(device)\n",
    "            past_trajectory = batch['past_trajectory'].to(device)\n",
    "            intent = batch['intent'].to(device)\n",
    "            future_trajectory = batch['future_trajectory'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(images, past_trajectory, intent)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, future_trajectory)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                # Move data to device\n",
    "                images = batch['images'].to(device)\n",
    "                past_trajectory = batch['past_trajectory'].to(device)\n",
    "                intent = batch['intent'].to(device)\n",
    "                future_trajectory = batch['future_trajectory'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = model(images, past_trajectory, intent)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(predictions, future_trajectory)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            # Calculate average validation loss\n",
    "            val_loss /= len(val_loader)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_e2e_driving_model.pth')\n",
    "            print(\"Best model saved!\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate the model and calculate metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_ade = 0.0\n",
    "    predictions_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # Move data to device\n",
    "            images = batch['images'].to(device)\n",
    "            past_trajectory = batch['past_trajectory'].to(device)\n",
    "            intent = batch['intent'].to(device)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model(images, past_trajectory, intent)\n",
    "            \n",
    "            # Calculate ADE if ground truth is available\n",
    "            if 'future_trajectory' in batch:\n",
    "                future_trajectory = batch['future_trajectory'].to(device)\n",
    "                batch_ade = torch.norm(predictions - future_trajectory, dim=2).mean().item()\n",
    "                total_ade += batch_ade\n",
    "            \n",
    "            # Store predictions for submission\n",
    "            for i, frame_name in enumerate(batch['frame_name']):\n",
    "                pred_traj = predictions[i].cpu().numpy()\n",
    "                predictions_dict[frame_name] = {\n",
    "                    'pos_x': pred_traj[:, 0],\n",
    "                    'pos_y': pred_traj[:, 1]\n",
    "                }\n",
    "    \n",
    "    # Calculate average ADE if ground truth was available\n",
    "    if 'future_trajectory' in next(iter(test_loader)):\n",
    "        avg_ade = total_ade / len(test_loader)\n",
    "        print(f\"Average ADE: {avg_ade:.4f}\")\n",
    "    \n",
    "    return predictions_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(predictions_dict, submission_file_base, authors, affiliation, account_name, method_name):\n",
    "    \"\"\"Create submission file in the required format.\"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(submission_file_base):\n",
    "        os.makedirs(submission_file_base)\n",
    "    \n",
    "    # Create frames with trajectory predictions\n",
    "    frame_predictions = []\n",
    "    for frame_name, pred in predictions_dict.items():\n",
    "        traj_pred = wod_e2ed_submission_pb2.TrajectoryPrediction(\n",
    "            pos_x=pred['pos_x'].tolist(),\n",
    "            pos_y=pred['pos_y'].tolist()\n",
    "        )\n",
    "        \n",
    "        frame_traj = wod_e2ed_submission_pb2.FrameTrajectoryPredictions(\n",
    "            frame_name=frame_name,\n",
    "            trajectory=traj_pred\n",
    "        )\n",
    "        \n",
    "        frame_predictions.append(frame_traj)\n",
    "    \n",
    "    # Create submission proto\n",
    "    submission = wod_e2ed_submission_pb2.E2EDChallengeSubmission(\n",
    "        predictions=frame_predictions,\n",
    "        submission_type=wod_e2ed_submission_pb2.E2EDChallengeSubmission.SubmissionType.E2ED_SUBMISSION,\n",
    "        authors=authors,\n",
    "        affiliation=affiliation,\n",
    "        account_name=account_name,\n",
    "        unique_method_name=method_name,\n",
    "        method_link=\"https://github.com/yourusername/waymo-e2e-driving\",\n",
    "        description=\"End-to-end vision-based autonomous driving model using multi-camera perception\",\n",
    "        uses_public_model_pretraining=True,\n",
    "        public_model_names=[\"ResNet18\"],\n",
    "        num_model_parameters=\"25M\"\n",
    "    )\n",
    "    \n",
    "    # Write submission to file\n",
    "    submission_file = os.path.join(submission_file_base, 'submission.pb')\n",
    "    with tf.io.gfile.GFile(submission_file, 'wb') as fp:\n",
    "        fp.write(submission.SerializeToString())\n",
    "    \n",
    "    print(f\"Submission file created at {submission_file}\")\n",
    "    \n",
    "    # Create tar.gz (this would need additional shell commands in practice)\n",
    "    print(\"To create tar.gz file, run the following commands:\")\n",
    "    print(f\"cd {os.path.dirname(submission_file_base)}\")\n",
    "    print(f\"tar cvf {os.path.basename(submission_file_base)}.tar {os.path.basename(submission_file_base)}\")\n",
    "    print(f\"gzip {os.path.basename(submission_file_base)}.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CURL_CA_BUNDLE'] = '/home/aaylen/Documents/Waymo-Challenge/cacert.pem'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/aaylen/Documents/Waymo-Challenge/token.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Dataset paths - replace with actual paths\n",
    "    DATASET_FOLDER = 'gs://waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "    TRAIN_FILES = tf.io.matching_files(os.path.join(DATASET_FOLDER, '*.tfrecord-*'))\n",
    "    VALIDATION_FILES = tf.io.matching_files(os.path.join(DATASET_FOLDER, '*.tfrecord-*'))\n",
    "    TEST_FILES = tf.io.matching_files(os.path.join(DATASET_FOLDER, '*.tfrecord-*'))\n",
    "\n",
    "    print(f\"Training files: {TRAIN_FILES}\")\n",
    "    # Create datasets\n",
    "    train_dataset = WaymoE2EDataset(TRAIN_FILES.numpy(), transform=transform)\n",
    "    val_dataset = WaymoE2EDataset(VALIDATION_FILES.numpy(), transform=transform)\n",
    "    test_dataset = WaymoE2EDataset(TEST_FILES.numpy(), transform=transform)\n",
    "\n",
    "    print(train_dataset[0])\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = E2EDrivingModel().to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_e2e_driving_model.pth'))\n",
    "    \n",
    "    # Evaluate model and get predictions\n",
    "    predictions_dict = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # Create submission\n",
    "    create_submission(\n",
    "        predictions_dict=predictions_dict,\n",
    "        submission_file_base='/tmp/WaymoE2ESubmission',\n",
    "        authors=['Your Name'],\n",
    "        affiliation='Your Organization',\n",
    "        account_name='your.email@example.com',\n",
    "        method_name='VisionE2EDriving'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://waymo_open_dataset_end_to_end_camera_v_1_0_0/*.tfrecord-*\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
