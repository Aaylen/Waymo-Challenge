{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waymo Open Dataset E2ED Visualization\n",
    "\n",
    "This notebook visualizes all camera images, ego vehicle intent, sequence number, and index number from the Waymo Open Dataset End-to-End Driving (E2ED) data. It also projects past trajectory states onto back cameras (side_left, side_right) and future states onto front cameras (front, front_left, front_right).\n",
    "\n",
    "**Dataset Details**:\n",
    "- Visualizes all cameras: front (1), front_left (2), front_right (3), side_left (4), side_right (5).\n",
    "- Past states (t = -4s to 0s) are shown on back cameras; future states (t = 0s to 5s) on front cameras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:40:32.655184: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 20:40:32.656527: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-24 20:40:32.683546: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-24 20:40:32.684131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 20:40:34.023297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.wdl_limited.camera.ops import py_camera_model_ops\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = 'gs://waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "\n",
    "TRAIN_FILES = os.path.join(DATASET_FOLDER, '*.tfrecord-*')\n",
    "VALIDATION_FILES = os.path.join(DATASET_FOLDER, '*.tfrecord-*')\n",
    "TEST_FILES = os.path.join(DATASET_FOLDER, '*.tfrecord-*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CURL_CA_BUNDLE'] = '/home/aaylen/Documents/Waymo-Challenge/cacert.pem'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/aaylen/Documents/Waymo-Challenge/token1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 20:40:40.851070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-04-24 20:40:40.851416: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-04-24 20:40:42.283656: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [315]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.io.matching_files(TRAIN_FILES)\n",
    "dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
    "dataset_iter = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:01:52.137860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [315]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d6cdf6eb1b7d4a8be6dac71f34e6cdb7-164\n",
      "b197472f28df9f18c22654a5b514082a-072\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for authentication\n",
    "os.environ['CURL_CA_BUNDLE'] = '/home/aaylen/Documents/Waymo-Challenge/cacert.pem'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/aaylen/Documents/Waymo-Challenge/token1.json'\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_FOLDER = 'gs://waymo_open_dataset_end_to_end_camera_v_1_0_0'\n",
    "TRAIN_FILES = os.path.join(DATASET_FOLDER, '*.tfrecord-*')\n",
    "\n",
    "# Initialize dataset\n",
    "filenames = tf.io.matching_files(TRAIN_FILES)\n",
    "dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
    "dataset_iter = dataset.as_numpy_iterator()\n",
    "\n",
    "# Retrieve one example (targeting the specified frame)\n",
    "target_frame_name = 'b197472f28df9f18c22654a5b514082a-072'\n",
    "data = None\n",
    "for bytes_example in dataset_iter:\n",
    "    frame_data = wod_e2ed_pb2.E2EDFrame()\n",
    "    frame_data.ParseFromString(bytes_example)\n",
    "    print(frame_data.frame.context.name)\n",
    "    if frame_data.frame.context.name == target_frame_name:\n",
    "        data = frame_data\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cameras(data: wod_e2ed_pb2.E2EDFrame):\n",
    "    \"\"\"Return all 8 camera images and calibrations.\"\"\"\n",
    "    image_list = []\n",
    "    calibration_list = []\n",
    "    camera_names = {\n",
    "        1: 'FRONT',\n",
    "        2: 'FRONT_LEFT',\n",
    "        3: 'FRONT_RIGHT',\n",
    "        4: 'SIDE_LEFT',\n",
    "        5: 'SIDE_RIGHT',\n",
    "        6: 'REAR_LEFT',\n",
    "        7: 'REAR',\n",
    "        8: 'REAR_RIGHT'\n",
    "    }\n",
    "    order = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    \n",
    "    for camera_name in order:\n",
    "        for index, image_content in enumerate(data.frame.images):\n",
    "            if image_content.name == camera_name:\n",
    "                calibration = data.frame.context.camera_calibrations[index]\n",
    "                image = tf.io.decode_image(image_content.image).numpy()\n",
    "                image_list.append((camera_names[camera_name], image))\n",
    "                calibration_list.append((camera_names[camera_name], calibration))\n",
    "                break\n",
    "    \n",
    "    return image_list, calibration_list\n",
    "\n",
    "def project_vehicle_to_image(vehicle_pose, calibration, points):\n",
    "    \"\"\"Projects from vehicle coordinate system to image with global shutter.\"\"\"\n",
    "    pose_matrix = np.array(vehicle_pose.transform).reshape(4, 4)\n",
    "    world_points = np.zeros_like(points)\n",
    "    for i, point in enumerate(points):\n",
    "        cx, cy, cz, _ = np.matmul(pose_matrix, [*point, 1])\n",
    "        world_points[i] = (cx, cy, cz)\n",
    "\n",
    "    extrinsic = tf.reshape(\n",
    "        tf.constant(list(calibration.extrinsic.transform), dtype=tf.float32), [4, 4])\n",
    "    intrinsic = tf.constant(list(calibration.intrinsic), dtype=tf.float32)\n",
    "    metadata = tf.constant([\n",
    "        calibration.width,\n",
    "        calibration.height,\n",
    "        open_dataset.CameraCalibration.GLOBAL_SHUTTER,\n",
    "    ], dtype=tf.int32)\n",
    "    camera_image_metadata = list(vehicle_pose.transform) + [0.0] * 10\n",
    "\n",
    "    return py_camera_model_ops.world_to_image(\n",
    "        extrinsic, intrinsic, metadata, camera_image_metadata, world_points).numpy()\n",
    "\n",
    "def draw_points_on_image(image, points, size, color=(255, 0, 0)):\n",
    "    \"\"\"Draws points on an image.\"\"\"\n",
    "    image_copy = image.copy()\n",
    "    for point in points:\n",
    "        if point[2] > 0:  # Check if point is valid (ok flag)\n",
    "            cv2.circle(image_copy, (int(point[0]), int(point[1])), size, color, -1)\n",
    "    return image_copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize All Cameras with Trajectories and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     images_with_points\u001b[38;5;241m.\u001b[39mappend((camera_name, image_with_points))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create top-down stitched view\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m top_down_image \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_top_down_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_camera_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_camera_calibrations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_waypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_waypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehicle_pose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Extract metadata\u001b[39;00m\n\u001b[1;32m     29\u001b[0m frame_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mname\n",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m, in \u001b[0;36mcreate_top_down_view\u001b[0;34m(images, calibrations, past_waypoints, future_waypoints, vehicle_pose, resolution, view_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m     uv \u001b[38;5;241m=\u001b[39m uv[valid]\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Copy pixel colors to canvas\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (px, py), (u, v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mzip\u001b[39m(canvas_x, canvas_y), uv\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m     59\u001b[0m         canvas[py, px] \u001b[38;5;241m=\u001b[39m image[v, u]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Project past and future waypoints onto the canvas\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "all_camera_images, all_camera_calibrations = get_all_cameras(data)\n",
    "\n",
    "past_waypoints = np.stack([data.past_states.pos_x, data.past_states.pos_y, np.zeros_like(data.past_states.pos_x)], axis=1)\n",
    "future_waypoints = np.stack([data.future_states.pos_x, data.future_states.pos_y, data.future_states.pos_z], axis=1)\n",
    "\n",
    "vehicle_pose = data.frame.images[0].pose\n",
    "\n",
    "images_with_points = []\n",
    "camera_names = [name for name, _ in all_camera_images]\n",
    "for i, (camera_name, image) in enumerate(all_camera_images):\n",
    "    calibration = next(calib for name, calib in all_camera_calibrations if name == camera_name)\n",
    "    if camera_name in ['SIDE_LEFT', 'SIDE_RIGHT', 'REAR', 'REAR_LEFT', 'REAR_RIGHT']:\n",
    "        waypoints_camera_space = project_vehicle_to_image(vehicle_pose, calibration, past_waypoints)\n",
    "        image_with_points = draw_points_on_image(image, waypoints_camera_space, size=15, color=(0, 255, 0))\n",
    "    elif camera_name in ['FRONT', 'FRONT_LEFT', 'FRONT_RIGHT']:\n",
    "        waypoints_camera_space = project_vehicle_to_image(vehicle_pose, calibration, future_waypoints)\n",
    "        image_with_points = draw_points_on_image(image, waypoints_camera_space, size=15, color=(255, 0, 0))\n",
    "    else:\n",
    "        image_with_points = image.copy()\n",
    "    images_with_points.append((camera_name, image_with_points))\n",
    "\n",
    "top_down_image = create_top_down_view(all_camera_images, all_camera_calibrations, past_waypoints, future_waypoints, vehicle_pose)\n",
    "\n",
    "frame_name = data.frame.context.name\n",
    "uuid, seq_num = frame_name.rsplit('-', 1)\n",
    "seq_num = int(seq_num)\n",
    "intent = wod_e2ed_pb2.EgoIntent.Intent.Name(data.intent)\n",
    "index_num = seq_num\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "axes = fig.add_subplot(2, 4, (1, 3))\n",
    "axes.imshow(top_down_image)\n",
    "axes.set_title('Top-Down Stitched View')\n",
    "axes.axis('off')\n",
    "\n",
    "for i, (camera_name, image) in enumerate(images_with_points):\n",
    "    ax = fig.add_subplot(2, 4, i+5)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(camera_name)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'Frame: {frame_name}\\nUUID: {uuid}\\nSeqNum: {seq_num}\\nIndex: {index_num}\\nIntent: {intent}', fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
